import torch
import torch.nn as nn
import torch.nn.functional as F
from utils import comm

from torch.cuda import amp

from networks.helpers import trunc_normal_

# matmul parallel
from distributed.mappings import (
    copy_to_parallel_region,
    gather_from_parallel_region,
    all_gather_from_parallel_region,
    reduce_from_parallel_region,
    reduce_scatter_to_parallel_region
)
from typing import Tuple
from .ringX_attn import ringX_attn_func
from .ring_attn import ring_attn_func

class DistributedMatmul(nn.Module):
    """Distributed Matrix Multiply
    Y = XW
    W is sharded in a 1D fashion: either row or col parallel
    W is a (in_dim, out_dim) size matrix when unsharded
    So shape of W is either (in_dim/n, out_dim) or (in_dim, out_dim/n)
    X is assumed sharded similarly to match the dimensions
    comm_act_name is an orthogonal comm used for sharding the activation 
    X using m procs (batch_seq/m, in_dim)
    """

    def __init__(
        self,
        inp_dim,
        out_dim,
        comm_inp_name,
        comm_out_name,
        comm_act_name='cp',
        bias=True,
    ):
        super(DistributedMatmul, self).__init__()

        # get sizes
        self.comm_inp_name = comm_inp_name
        self.comm_out_name = comm_out_name
        comm_inp_size = comm.get_size(self.comm_inp_name)
        comm_out_size = comm.get_size(self.comm_out_name)

        assert (
            not (comm_inp_size > 1 and comm_out_size > 1)
        ), "Error, weights are sharded in a 2D fashion, not supported currently"
        assert (
            inp_dim % comm_inp_size == 0
        ), f"Error, the size of input feature dim ({inp_dim}) has to be evenly divisible by the input feature comm dim ({comm_inp_size})"
        assert (
            out_dim % comm_out_size == 0
        ), f"Error, the size of output feature dim ({out_dim}) has to be evenly divisible by the output feature comm dim ({comm_out_size})"

        # compute reduced dims
        inp_dim_local = inp_dim // comm_inp_size
        out_dim_local = out_dim // comm_out_size

        # parameters
        self.weight = nn.Parameter(torch.ones(out_dim_local, inp_dim_local))
        self.weight.is_shared_mp = [comm_act_name] # weights are sharded in tp but shared across cp
        self.weight.mark_for_reduction = [comm_act_name] # shared weights must be additionally reduced
        if bias:
            self.bias = nn.Parameter(torch.ones(1, 1, out_dim_local))
            # if inp dim of W is sharded, then the bias is shared across this group and also
            # shared in cp grp
            self.bias.is_shared_mp = [self.comm_inp_name, comm_act_name]
            self.bias.mark_for_reduction = [comm_act_name] # shared bias must be additionally reduced

        # init weights
        self._init_weights()

    def _init_weights(self):
        trunc_normal_(self.weight, std=0.02)
        if hasattr(self, "bias"):
            nn.init.constant_(self.bias, 0.0)

    # since this method is full of custom autograd, it cannot be jitted from torch frontend.
    @torch.jit.ignore
    def forward(self, x):
        x_cp = copy_to_parallel_region(x, self.comm_out_name)
        # don't add bias (else allreduce will add it too often)
        x_loc = F.linear(x_cp, self.weight, bias=None) 
        x_out = reduce_from_parallel_region(x_loc, self.comm_inp_name)
        if hasattr(self, "bias"):
            x_out = x_out + self.bias
        return x_out


class DistributedMLP(nn.Module):
    """Distributed MLP layer
    Currently implements 1D tensor parallelism
    """

    def __init__(
        self,
        in_features,
        hidden_features=None,
        out_features=None,
        comm_tp_name='tp',
        comm_cp_name='cp',
        act_layer=nn.GELU,
        drop=0.0,
    ):

        super(DistributedMLP, self).__init__()
        out_features = out_features or in_features
        hidden_features = hidden_features or in_features

        self.fc1 = DistributedMatmul(
            in_features,
            hidden_features,
            comm_inp_name=None,
            comm_out_name=comm_tp_name,
            comm_act_name=comm_cp_name,
            bias=True,
        )

        self.fc2 = DistributedMatmul(
            hidden_features,
            out_features,
            comm_inp_name=comm_tp_name,
            comm_out_name=None,
            comm_act_name=comm_cp_name,
            bias=True,
        )

        self.act = act_layer()
        self.drop = nn.Dropout(drop)

    def forward(self, x):
        x = self.fc1(x)
        x = self.act(x)
        x = self.drop(x)
        x = self.fc2(x)
        x = self.drop(x)
        return x


class DistributedAttention(nn.Module):
    """Distributed Attention layer"""

    def __init__(
        self,
        dim,
        comm_tp_name='tp',
        comm_cp_name='cp',
        cp_shapes=None,
        num_heads=8,
        qkv_bias=False,
        attn_type='megatron',
        attn_drop=0.0,
        proj_drop=0.0
    ):

        super(DistributedAttention, self).__init__()

        assert dim % num_heads == 0, "dim should be divisible by num_heads"

        self.num_heads = num_heads
        assert (
            num_heads % comm.get_size(comm_tp_name) == 0
        ), "heads are not evenly split across TP model ranks"

        self.num_heads_local = num_heads // comm.get_size(comm_tp_name)
        self.head_dim = dim // self.num_heads
        self.scale = (dim // self.num_heads) ** -0.5
        self.fused_attn = True

        self.comm_tp_name = comm_tp_name
        self.comm_cp_name = comm_cp_name
        self.cp_shapes = cp_shapes

        # qkv is col parallel in the weights
        self.q = DistributedMatmul(dim, dim,
            comm_inp_name=None, 
            comm_out_name=comm_tp_name, 
            bias=qkv_bias,
            comm_act_name=comm_cp_name
        )
        self.k = DistributedMatmul(dim, dim,
            comm_inp_name=None, 
            comm_out_name=comm_tp_name, 
            bias=qkv_bias,
            comm_act_name=comm_cp_name
        )
        self.v = DistributedMatmul(dim, dim,
            comm_inp_name=None, 
            comm_out_name=comm_tp_name, 
            bias=qkv_bias,
            comm_act_name=comm_cp_name
        )
        self.attn_drop = nn.Dropout(attn_drop)

        # proj is row parallel in the weights
        self.proj = DistributedMatmul(
            dim, dim, comm_inp_name=comm_tp_name, comm_out_name=None,
            comm_act_name=comm_cp_name
        )
        self.proj_drop = nn.Dropout(proj_drop)
        self.cp_group = comm.get_group(self.comm_cp_name)
        self.attn_type = attn_type

    def forward(self, x):

        # note: N is local sequence shard if CP is on
        B, N, C = x.shape
       
        q = self.q(x).reshape(B, N, self.num_heads_local, self.head_dim).to(torch.bfloat16) #.permute(0, 2, 1, 3)
        k = self.k(x).reshape(B, N, self.num_heads_local, self.head_dim).to(torch.bfloat16) #.permute(0, 2, 1, 3)
        v = self.v(x).reshape(B, N, self.num_heads_local, self.head_dim).to(torch.bfloat16) #.permute(0, 2, 1, 3)
        dtype = x.dtype

        if self.attn_type == 'ringX':
            #q = q.to(torch.bfloat16)
            #k = k.to(torch.bfloat16)
            #v = v.to(torch.bfloat16)
            x = ringX_attn_func(q, k, v, causal=False, dropout_p=self.attn_drop.p, group=self.cp_group)
            x = x.reshape(B, N, self.num_heads_local * self.head_dim).to(dtype)
        elif self.attn_type == 'ring':
            #q = q.to(torch.bfloat16)
            #k = k.to(torch.bfloat16)
            #v = v.to(torch.bfloat16)
            x = ring_attn_func(q, k, v, causal=False, dropout_p=self.attn_drop.p, group=self.cp_group)
            x = x.reshape(B, N, self.num_heads_local * self.head_dim).to(dtype)
        elif self.attn_type == 'megatron':
            q = q.permute(0, 2, 1, 3)
            k = k.permute(0, 2, 1, 3)
            v = v.permute(0, 2, 1, 3)
            k = all_gather_from_parallel_region(k, dim=2, shapes=self.cp_shapes, comm_name=self.comm_cp_name)
            v = all_gather_from_parallel_region(v, dim=2, shapes=self.cp_shapes, comm_name=self.comm_cp_name)

            if self.fused_attn:
                x = F.scaled_dot_product_attention(
                    q,
                    k,
                    v,
                    dropout_p=self.attn_drop.p,
                )
            else:
                q = q * self.scale
                attn = q @ k.transpose(-2, -1)
                attn = attn.softmax(dim=-1)
                attn = self.attn_drop(attn)
                x = attn @ v
            # transpose back
            x = x.transpose(1, 2).reshape(B, N, self.num_heads_local * self.head_dim).to(dtype)
        else:
            print("attn_type can only be ringX, ring, or gather")
            exit(1)
        # this is distributed again
        x = self.proj(x)

        # generally we have to be super careful with dropout layers, since
        # those are normalized over the dropouts. That would need to be reduced across nodes
        x = self.proj_drop(x)

        return x

class DistributedLayerNorm(nn.Module):
    """ 
    Distributed layer norm layer 
    Sequence parallel only
    """
    def __init__(self,
                 normalized_shape,
                 eps=1e-05,
                 elementwise_affine=True,
                 bias=True,
                 device=None,
                 dtype=None,
                 comm_tp_name='tp',
                 comm_cp_name='cp'):
        super(DistributedLayerNorm, self).__init__()
         
        self.norm = nn.LayerNorm(normalized_shape,
                                 eps=eps,
                                 elementwise_affine=elementwise_affine,
                                 bias=bias, device=device, dtype=dtype)

        if elementwise_affine:
            # affine weights need additional allreduce and are shared 
            # across all groups
            self.norm.weight.is_shared_mp = [comm_tp_name, comm_cp_name]
            self.norm.weight.mark_for_reduction = [comm_cp_name]
            if bias:
                self.norm.bias.is_shared_mp = [comm_tp_name, comm_cp_name]
                self.norm.bias.mark_for_reduction = [comm_cp_name]


    def forward(self, x):
        return self.norm(x)
